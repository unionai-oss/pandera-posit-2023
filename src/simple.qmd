---
title: "Validating and Testing R DataFrames with `pandera` and `reticulate`"
author: Niels Bantilan
date: Sept 30, 2023
theme: default
format:
  revealjs:
    slide-number: true
    mermaid:
      theme: default
---

![](static/hook_data_scientist_job.png){width=500}

::: {.notes}
In October 2012, data science was named the sexiest job by the Harvard Business
Review. But you know what isn't sexy?
:::

## Dealing with invalid data ğŸ˜­

![](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExajh4dTl3d2Z0YTRtaTlkeHgxd2trcDJsb2d2YmZzbW1kOTczMndzdiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/XCxcmEQWxDdc8qsd2R/giphy.gif){width=500}

::: {.notes}
Dealing with Invalid Data, which can often feel like a losing battle when you
don't realize something that corrupted or otherwise incorrect data has passed
through your pipeline.
:::

## Data validation is important...

... but tedious ğŸ˜‘

. . .

> "Garbage in, garbage out"

. . .

> "Data-centric machine learning"

. . .

> "Data as code"

. . .

*"But I just want to train my model!"*

::: {.notes}
We all know that data quality is super important.

...

My job, in this talk, is not to convince you that data validation is the most attractive
part of data science, but I do want to convince you that...
:::

## Data validation can be fun ğŸ‰

. . .

**Data validation is like unit testing for your data**

. . .

```
$ run pipeline
```

- âœ… `dataset_x_validation`
- âœ… `dataset_y_validation`
- âœ… `dataset_z_validation`

. . .

### âœ¨ğŸªâœ¨ {.larger}

::: {.notes}
And the way you do that is to reframe data validation as unit tests for your data.

This way when you run your data pipelines you can get that little dopamine hit
knowing that your datasets are valid.
:::

## A day in the life of a data scientist {.smaller}

```{mermaid}
%%| fig-width: 10
%%| fig-height: 5

flowchart LR
  A[clean data] --> S[split data]
  S --> B[train model]
  B --> C[evaluate model]
  S --> C
```

::: {.notes}
Let me illustrate this with an all too familiar story.

In one of my past jobs I had to train a model, so my pipeline looked something
like this:
:::

## A day in the life of a data scientist {.smaller}

```{mermaid}
%%| fig-width: 10
%%| fig-height: 5

flowchart LR
  A[clean data] --> S[split data]
  S --> B[train model]
  B --> C[evaluate model]
  S --> C

  style B fill:#e0e0e0,stroke:#333
```

::: {.notes}
Now the training step can take a long time to complete, so I had to wait a few
days for that to complete...
:::

## A day in the life of a data scientist {.smaller}

```{mermaid}
%%| fig-width: 10
%%| fig-height: 5

flowchart LR
  A[clean data] --> S[split data]
  S --> B[train model]
  B --> C[evaluate model]
  S --> C

  style B fill:#8cffb4,stroke:#333
  style C fill:#ff8c8c,stroke:#333
```

::: {.notes}
Only to find that a bug in the way I was creating the test data caused my
evaluation step to fail.

Had I made some assertions about what the test data should look like at the
split data step, I would have caught this data bug early before wasting all of
that time training the model.
:::


## A day in the life of a data scientist {.smaller}

**Unit Tests**

- âœ… `test_clean_data`
- âœ… `test_split_data`
- âœ… `test_train_model`
- âœ… `test_evaluate_model`

. . .

**Data Tests**

- âœ… `validate_cleaned_data`
- âœ… `validate_split_data`
- âœ… `validate_training_data`
- âœ… `validate_metrics_data`


::: {.notes}
So just like have unit tests for your functions, you can also define
data tests for your data as it progresses through your data and modeling pipelines.

This way you can get that little dopamine hit when your data validations pass
with all of those green checkmarks.

I can tell you from experience, it really makes you feel safe and confident
knowing that your data is as you expect it to be...
:::

## A day in the life of a data scientist {.smaller}

**Unit Tests**

- âœ… `test_clean_data`
- âœ… `test_split_data`
- âœ… `test_train_model`
- âŒ `test_evaluate_model`

**Data Tests**

- âœ… `validate_cleaned_data`
- âŒ `validate_split_data`
- âœ… `validate_training_data`
- âœ… `validate_metrics_data`


::: {.notes}
or that something went wrong and you know exactly where to start looking for
a bug in your data.

But this benefit doesn't come for free...
:::

## The data validation journey

::: {.notes}
Like many things in life, the journey is as important as the destination.
:::

::: {.notes}
I built pandera to lower the barrier to maintaining data hygiene
:::

## How `pandera` works

::: {.notes}
Now this is all well and good if you're using Python, but what about if you're
using R?
:::

## Using `pandera` in R via `reticulate`

::: {.notes}
I went down this path with some trepidation that pandera will break in R, but
it just works!
:::

## Get `pandera` set up on your stack in 5 minutes

---

[insert data validation meme here]

::: {.notes}
By understanding what counts as valid data, creating schemas for them, and using
these schemas across your Python and R stack, you get a single source of truth
for your data contracts. These contracts serve as data documentation for you and
your team *and* run-time validators for your data in both development and production
contexts. `pandera` lowers the barrier to maintaining data hygiene so you can spend
less time worrying about the correctness of your data and more time analyzing, visualizing,
and modeling them.
:::
